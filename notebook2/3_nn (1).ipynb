{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "sys.path.append(r\"..\")\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,f1_score,recall_score\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.client import device_lib\n",
    "import logging\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(\n",
    "    filename='my.log', \n",
    "    level=logging.DEBUG, \n",
    "    format=LOG_FORMAT)\n",
    "\n",
    "path_build = \"./build/\"\n",
    "path_embed = \"./embed/\"\n",
    "path_list = \"./feature_series/\"\n",
    "path_mnt = \"./mnt/\"\n",
    "path_sub = \"./sub/\"\n",
    "print(tf.__version__)\n",
    "#print(tf.test.is_built_with_gpu_support)\n",
    "#print(tf.test.is_gpu_available())\n",
    "#print(device_lib.list_local_devices())\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#对需要进行限制的GPU进行设置\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#                                                       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "# gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_id_em = load_pickle(f\"{path_embed}/creative_id_w2v_matrix.pkl\")\n",
    "#ad_id_em = load_pickle(f\"{path_embed}/ad_id_w2v_matrix.pkl\")\n",
    "advertiser_id_em = load_pickle(f\"{path_embed}/advertiser_id_w2v_matrix.pkl\")\n",
    "product_id_em = load_pickle(f\"{path_embed}/product_id_w2v_matrix.pkl\")\n",
    "industry_em = load_pickle(f\"{path_embed}/industry_w2v_matrix.pkl\")\n",
    "product_category_em = load_pickle(f\"{path_embed}/product_category_w2v_matrix.pkl\")\n",
    "# click_times_em = load_pickle(f\"{path_embed}/click_times_w2v_matrix2.pkl\")\n",
    "# time_em = load_pickle(f\"{path_embed}/time_w2v_matrix2.pkl\")\n",
    "\n",
    "#creative_id_times_em = load_pickle(f\"{path_embed}/creative_id_times_w2v_matrix.pkl\")\n",
    "#ad_id_times_em = load_pickle(f\"{path_embed}/ad_id_times_w2v_matrix3.pkl\")\n",
    "#product_id_times_em = load_pickle(f\"{path_embed}/product_id_times_w2v_matrix.pkl\")\n",
    "#advertiser_id_times_em = load_pickle(f\"{path_embed}/advertiser_id_times_w2v_matrix.pkl\")\n",
    "#product_category_times_em = load_pickle(f\"{path_embed}/product_category_times_w2v_matrix.pkl\")\n",
    "#industry_times_em = load_pickle(f\"{path_embed}/industry_times_w2v_matrix.pkl\")\n",
    "\n",
    "# creative_id_t_em = load_pickle(f\"{path_save}/creative_id_t_w2v_matrix2.pkl\")\n",
    "# # ad_id_t_em = load_pickle(f\"{path_save}/ad_id_t_w2v_matrix2.pkl\")\n",
    "# product_id_t_em = load_pickle(f\"{path_save}/product_id_t_w2v_matrix2.pkl\")\n",
    "# advertiser_id_t_em = load_pickle(f\"{path_save}/advertiser_id_t_w2v_matrix2.pkl\")\n",
    "# product_category_t_em = load_pickle(f\"{path_save}/product_category_t_w2v_matrix2.pkl\")\n",
    "# industry_t_em = load_pickle(f\"{path_save}/industry_t_w2v_matrix2.pkl\")\n",
    "\n",
    "#time_clicktimes_em = load_pickle(f\"{path_embed}/time_clicktimes_w2v_matrix2.pkl\")\n",
    "#time_creativeids_em = load_pickle(f\"{path_embed}/time_creativeids_w2v_matrix2.pkl\")\n",
    "\n",
    "w2v_features = [\n",
    "    {'name':'creative_id', 'size':256, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':creative_id_em},\n",
    "    #{'name':'ad_id', 'size':128, 'windows':5, 'min_count':1, 'version':1, 'max_len':128, 'em':ad_id_em},\n",
    "    {'name':'advertiser_id', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':advertiser_id_em},\n",
    "    {'name':'product_id', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':product_id_em},\n",
    "    {'name':'industry', 'size':32, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':industry_em},\n",
    "    {'name':'product_category', 'size':16, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':product_category_em},\n",
    "#     {'name':'time', 'size':16, 'windows':5, 'min_count':1, 'version':1, 'max_len':128, 'em':time_em},\n",
    "#     {'name':'click_times', 'size':8, 'windows':5, 'min_count':1, 'version':1, 'max_len':128, 'em':click_times_em},\n",
    "    \n",
    "    #{'name':'creative_id_times', 'size':256, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':creative_id_times_em},\n",
    "    #{'name': 'ad_id_times', 'size': 128, 'windows': 10, 'min_count': 1, 'version': 1, 'max_len':128, 'em':ad_id_times_em},\n",
    "    #{'name':'product_id_times', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':product_id_times_em},\n",
    "    #{'name':'advertiser_id_times', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':advertiser_id_times_em},\n",
    "    #{'name':'product_category_times', 'size':32, 'windows':10, 'min_count':1, 'version':1,'max_len':128, 'em':product_category_times_em},\n",
    "    #{'name':'industry_times', 'size':32, 'windows':10, 'min_count':1, 'version':1,'max_len':128, 'em':industry_times_em},   \n",
    "    \n",
    "    \n",
    "    #{'name':'time_clicktimes', 'size':91, 'windows':10, 'min_count':1, 'version':2, 'vocab_size':5000,'max_len':128,'em':time_clicktimes_em},\n",
    "    #{'name':'time_creativeids', 'size':91, 'windows':10, 'min_count':1, 'version':2,'vocab_size':5000, 'max_len':128,'em': time_creativeids_em},\n",
    "    \n",
    "]\n",
    "dense_features = ['creative_id_len', 'ad_id_len', 'product_id_len', 'product_category_len', 'advertiser_len', 'industry_len','time_len',\n",
    "                  'mean_clicktimes', 'max_clicktimes', 'min_clicktimes', 'mean_time', 'max_time', 'min_time']  \n",
    "dense_features = []\n",
    "base_features = ['product_id', 'product_category', 'advertiser_id', 'industry']\n",
    "#base_features = ['creative_id', 'ad_id']\n",
    "for fea in base_features:\n",
    "    for g in [1]:\n",
    "        dense_features.append(f'mean_prob_{fea}_gender_{g}')\n",
    "        dense_features.append(f'max_prob_{fea}_gender_{g}')\n",
    "        dense_features.append(f'min_prob_{fea}_gender_{g}')\n",
    "        dense_features.append(f'var_prob_{fea}_gender_{g}')\n",
    "    for a in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        dense_features.append(f'mean_prob_{fea}_age_{a}')\n",
    "        dense_features.append(f'max_prob_{fea}_age_{a}')\n",
    "        dense_features.append(f'min_prob_{fea}_age_{a}')\n",
    "        dense_features.append(f'var_prob_{fea}_age_{a}')\n",
    "dense_features = []\n",
    "\n",
    "#print(creative_id_em.shape,ad_id_em.shape,advertiser_id_em.shape,product_id_em.shape)\n",
    "#print(industry_em.shape,product_category_em.shape,click_times_em.shape,time_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(units,num_classes,w2v_features, dense_features):\n",
    "    inputs_dict = dict()\n",
    "    embed_layer_list = []\n",
    "    click_times_input = keras.Input(shape=(128,), name='click_times')\n",
    "    inputs_dict['click_times'] = click_times_input\n",
    "    \n",
    "    click_times_input_exp = tf.expand_dims(click_times_input, axis=2)\n",
    "    print(click_times_input_exp)\n",
    "    for w2v_f in w2v_features:\n",
    "        em_name = w2v_f['name']\n",
    "        if em_name == 'click_times':continue\n",
    "        em_size = w2v_f['em'].shape[0]\n",
    "        em_dim = w2v_f['size']\n",
    "        em_m = w2v_f['em']\n",
    "        max_len = w2v_f['max_len']\n",
    "        \n",
    "        print(em_name)    \n",
    "        inputs = keras.Input(shape=(max_len,), name=em_name)\n",
    "        inputs_dict[em_name] = inputs\n",
    "        embed_layer_list.append(\n",
    "            keras.layers.multiply([keras.layers.Embedding(em_size, em_dim, input_length=max_len, trainable=False, weights=[em_m],mask_zero=True)(inputs),\n",
    "                                   tf.tile(click_times_input_exp, [1,1,em_dim])]))\n",
    "#         embed_layer_list.append( keras.layers.Embedding(\n",
    "#                 em_size, em_dim, input_length=max_len, trainable=False, weights=[em_m],mask_zero=True)(inputs))\n",
    "    embed_output = keras.layers.concatenate(embed_layer_list, axis=-1)\n",
    "    #embed_output = keras.layers.Conv1D(512, 5, padding='same', kernel_initializer='normal', activation='relu')(embed_output)\n",
    "    \n",
    "    lstm_output0 = keras.layers.Bidirectional(keras.layers.LSTM(units,return_sequences=True))(embed_output)\n",
    "    \n",
    "    lstm_output = keras.layers.Bidirectional(keras.layers.LSTM(units,return_sequences=True))(lstm_output0)\n",
    "    lstm_output = keras.layers.Bidirectional(keras.layers.LSTM(units,return_sequences=True))(lstm_output)\n",
    "    lstm_output = keras.layers.Bidirectional(keras.layers.LSTM(units,return_sequences=True))(lstm_output)\n",
    "    lstm_output = keras.layers.Bidirectional(keras.layers.LSTM(units,return_sequences=True))(lstm_output)\n",
    "    lstm_output = keras.layers.Add()([lstm_output,lstm_output0])\n",
    "    #lstm_output = layers.GlobalMaxPooling1D()(lstm_output)\n",
    "                                     \n",
    "    \n",
    "    lstm_output = layers.concatenate([layers.GlobalAveragePooling1D()(lstm_output),\n",
    "                                      layers.GlobalMaxPooling1D()(lstm_output),\n",
    "                                     ], axis=-1)\n",
    "    #lstm_output = layers.BatchNormalization()(lstm_output)\n",
    "    lstm_output = layers.Dropout(0.3)(lstm_output)\n",
    "    \n",
    "    fc = keras.layers.Dense(units, activation='relu')(lstm_output)\n",
    "    #数值型特征\n",
    "    numeric_list = []\n",
    "    for den_f in dense_features:\n",
    "        inputs = keras.Input(shape=(1,), name=den_f)\n",
    "        inputs_dict[den_f] = inputs\n",
    "        numeric_list.append(inputs)   \n",
    "    if dense_features != []:\n",
    "        numeric_output = keras.layers.concatenate(numeric_list, axis=-1)\n",
    "        numeric_output_bn = layers.BatchNormalization()(numeric_output)\n",
    "        lstm_numeric_output = keras.layers.concatenate([fc,numeric_output_bn], axis=-1)\n",
    "    else:\n",
    "        lstm_numeric_output = fc\n",
    "    \n",
    "    \n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(lstm_numeric_output)\n",
    "    \n",
    "\n",
    "        \n",
    "    model = keras.Model(inputs=inputs_dict, outputs=outputs)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.001),\n",
    "              loss = keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_fn(feature_dict, label=None, epochs=5, shuffle=True, batch_size=64, fit_key='train'):\n",
    "#     if fit_key == 'train':\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices((feature_dict, label))\n",
    "#     else:\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices((feature_dict))\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(100*batch_size)\n",
    "#     dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "def input_fn(feature_dict, label=None, epochs=5, shuffle=True, batch_size=64, fit_key='train'):\n",
    "    if fit_key == 'train':\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((feature_dict, label))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((feature_dict))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100*batch_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# def meta_dict_gen_train():\n",
    "#     for i in range(2700000):\n",
    "#         ls = {}\n",
    "#         for key, val in feature_dict.items():\n",
    "#             ls[key] = val[i]\n",
    "#         yield ls\n",
    "    \n",
    "    \n",
    "# def input_fn(feature_dict, label=None, epochs=5, shuffle=True, batch_size=64, fit_key='train'):\n",
    "#     if fit_key == 'train':\n",
    "#         dataset = tf.data.Dataset.from_generator(\n",
    "#             meta_dict_gen_train,\n",
    "#             output_types=({k: tf.float32 for k in feature_dict}, tf.int32),\n",
    "#         )\n",
    "#     else:\n",
    "#         dataset = tf.data.Dataset.from_generator(\n",
    "#             meta_dict_gen_train,\n",
    "#             output_types=({k: tf.float32 for k in feature_dict},),\n",
    "#         )\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(100*batch_size)\n",
    "#     dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "\n",
    "\n",
    "# ems = dict()\n",
    "# with tf.device(\"/CPU:0\"):\n",
    "#     for w in w2v_features:\n",
    "#         ems[w['name']] = tf.constant(w['em'])\n",
    "# def data_embedding_lookup(X):\n",
    "#     for f in X:\n",
    "#         X[f] = tf.nn.embedding_lookup(ems[f],X[f])\n",
    "#     return X\n",
    "\n",
    "# def input_fn(feature_dict, label=None, epochs=5, shuffle=True, batch_size=64, fit_key='train'):\n",
    "#     if fit_key == 'train':\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices((feature_dict, label)).map(lambda X,y:(data_embedding_lookup(X),y), num_parallel_calls=8)\n",
    "#     else:\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices((feature_dict)).map(lambda X:(data_embedding_lookup(X)), num_parallel_calls=8)\n",
    "#     #print(dataset)\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(100*batch_size)\n",
    "#     dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    \n",
    "#     return dataset.map(map_func=lambda x,y:(x,y), num_parallel_calls=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creative_id\n",
      "advertiser_id\n",
      "product_id\n",
      "industry\n",
      "product_category\n",
      "click_times\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_pickle(f\"{path_build}/train_user.pkl\")\n",
    "users.sort_values(by=['user_id'], ascending=[True], inplace=True)\n",
    "users = users.reset_index(drop=True)\n",
    "\n",
    "users_mnt = pd.read_pickle(f\"{path_mnt}/user_mnts.pkl\")\n",
    "w2v_features.append({'name':'click_times', 'max_len':128}) #添加click_times\n",
    "fold_train = False\n",
    "train_split = [0,2700000]\n",
    "val_split = [2700000, 3000000]\n",
    "test_split = [3000000]\n",
    "train_feature_dict = dict()\n",
    "val_feature_dict = dict()\n",
    "test_feature_dict = dict()\n",
    "if fold_train:\n",
    "    kfolder = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "    kfold = kfolder.split(user_ids[0:3000000])\n",
    "    fold_index = 2\n",
    "    FLOD_TRAIN = 1\n",
    "    for train_index, vali_index in kfold:\n",
    "        if fold_index <= FLOD_TRAIN:\n",
    "            fold_index += 1\n",
    "            continue\n",
    "        print(train_index, vali_index)\n",
    "        break\n",
    "    \n",
    "    for fea in w2v_features:\n",
    "        name = fea['name']\n",
    "        max_length = fea['max_len']\n",
    "        print(name)\n",
    "        user_ids = np.load(f\"{path_list}{name}_list_int.npy\", allow_pickle=True)        \n",
    "        train_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[train_index],value = 0,padding = 'post',maxlen = max_length )\n",
    "        val_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[vali_index],value = 0,padding = 'post',maxlen = max_length )\n",
    "        test_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[test_split[0]:],value = 0,padding = 'post',maxlen = max_length )\n",
    "    for fea in dense_features:\n",
    "        print(fea)\n",
    "        train_feature_dict[fea] = users_mnt[fea][train_index]\n",
    "        val_feature_dict[fea] = users_mnt[fea][vali_index]\n",
    "        test_feature_dict[fea] = users_mnt[fea][test_split[0]:]\n",
    "\n",
    "    gender_train_label = np.array(users['gender'][train_index])\n",
    "    gender_val_label = np.array(users['gender'][vali_index])\n",
    "\n",
    "    age_train_label = np.array(users['age'][train_index])\n",
    "    age_val_label = np.array(users['age'][vali_index])\n",
    "    \n",
    "else:\n",
    "    for fea in w2v_features:\n",
    "        name = fea['name']\n",
    "        max_length = fea['max_len']\n",
    "        print(name)\n",
    "        user_ids = np.load(f\"{path_list}{name}_list_int.npy\", allow_pickle=True)\n",
    "        train_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[train_split[0]:train_split[1]],value = 0,padding = 'post',maxlen = max_length )\n",
    "        val_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[val_split[0]:val_split[1]],value = 0,padding = 'post',maxlen = max_length )\n",
    "        test_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[test_split[0]:],value = 0,padding = 'post',maxlen = max_length )\n",
    "        \n",
    "    for fea in dense_features:\n",
    "        print(fea)\n",
    "        train_feature_dict[fea] = users_mnt[fea][train_split[0]:train_split[1]]\n",
    "        val_feature_dict[fea] = users_mnt[fea][val_split[0]:val_split[1]]\n",
    "        test_feature_dict[fea] = users_mnt[fea][test_split[0]:]\n",
    "\n",
    "    gender_train_label = np.array(users['gender'][train_split[0]:train_split[1]])\n",
    "    gender_val_label = np.array(users['gender'][val_split[0]:val_split[1]])\n",
    "\n",
    "    age_train_label = np.array(users['age'][train_split[0]:train_split[1]])\n",
    "    age_val_label = np.array(users['age'][val_split[0]:val_split[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "def train_gender():\n",
    "    num_classes = 2\n",
    "    units = 128\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        gender_model = lstm_model(units, num_classes, w2v_features, dense_features)\n",
    "    train_dataset = input_fn(train_feature_dict, gender_train_label-1, epochs=5, shuffle=True, batch_size=512)\n",
    "    val_dataset = input_fn(val_feature_dict, gender_val_label-1, epochs=1, shuffle=False, batch_size=1024)\n",
    "    early_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0,\n",
    "                                  patience=3, verbose=0, mode='auto',\n",
    "                                  baseline=None, restore_best_weights=True)\n",
    "    gender_model.fit(train_dataset, validation_data=val_dataset, epochs=100, callbacks = [early_stopping])\n",
    "\n",
    "    test_dataset = input_fn(test_feature_dict, epochs=1, shuffle=False, batch_size=1024, fit_key='predict')\n",
    "    gender_prob = gender_model.predict(test_dataset)\n",
    "    gender_val_prob = gender_model.predict(val_dataset)\n",
    "    print(gender_prob.shape,gender_val_prob.shape)\n",
    "    tune_weight = search_weight(gender_val_label-1, gender_val_prob, init_weight=[1.0]*2,class_num=2, step=0.001)\n",
    "\n",
    "    gender_prob_tune = np.array(tune_weight)*gender_prob\n",
    "    gender_pre = np.argmax(gender_prob_tune,axis=1) + 1\n",
    "    np.save(f\"{path_sub}/val_gender_prob.npy\", gender_val_prob)\n",
    "    np.save(f\"{path_sub}/gender_prob.npy\", gender_prob)\n",
    "from multiprocessing import Process\n",
    "p = Process(target=train_gender)\n",
    "p.start()\n",
    "p.join() # 进程结束后，GPU显存会自动释放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_1:0\", shape=(None, 128, 1), dtype=float32)\n",
      "creative_id\n",
      "advertiser_id\n",
      "product_id\n",
      "industry\n",
      "product_category\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "click_times (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "creative_id (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 128, 1)]     0           click_times[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "advertiser_id (InputLayer)      [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_id (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "industry (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_category (InputLayer)   [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 128, 256)     1138104832  creative_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_5 (TensorFlowO [(None, 128, 256)]   0           tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 128, 64)      4030080     advertiser_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_6 (TensorFlowO [(None, 128, 64)]    0           tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 128, 64)      2836352     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_7 (TensorFlowO [(None, 128, 64)]    0           tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 128, 32)      10880       industry[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_8 (TensorFlowO [(None, 128, 32)]    0           tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 128, 16)      368         product_category[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_9 (TensorFlowO [(None, 128, 16)]    0           tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 128, 256)     0           embedding_16[0][0]               \n",
      "                                                                 tf_op_layer_Tile_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 128, 64)      0           embedding_17[0][0]               \n",
      "                                                                 tf_op_layer_Tile_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 128, 64)      0           embedding_18[0][0]               \n",
      "                                                                 tf_op_layer_Tile_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 128, 32)      0           embedding_19[0][0]               \n",
      "                                                                 tf_op_layer_Tile_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 128, 16)      0           embedding_20[0][0]               \n",
      "                                                                 tf_op_layer_Tile_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 432)     0           multiply_3[0][0]                 \n",
      "                                                                 multiply_4[0][0]                 \n",
      "                                                                 multiply_5[0][0]                 \n",
      "                                                                 multiply_6[0][0]                 \n",
      "                                                                 multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 128, 256)     574464      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128, 256)     394240      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 128, 256)     394240      bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 128, 256)     394240      bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 128, 256)     394240      bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 256)     0           bidirectional_9[0][0]            \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,147,200,890\n",
      "Trainable params: 2,218,378\n",
      "Non-trainable params: 1,144,982,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "units = 128\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# #devices=[\"/gpu:0\", \"/gpu:1\"]\n",
    "#with mirrored_strategy.scope():\n",
    "age_model = lstm_model(units, num_classes, w2v_features, dense_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "# train_dataset = input_fn(train_feature_dict, age_train_label-1, epochs=6, shuffle=True, batch_size=512)\n",
    "# val_dataset = input_fn(val_feature_dict, age_val_label-1, epochs=1, shuffle=False, batch_size=1024)\n",
    "\n",
    "# early_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0,\n",
    "#                               patience=3, verbose=0, mode='auto',\n",
    "#                               baseline=None, restore_best_weights=True)\n",
    "\n",
    "# age_model.fit(train_dataset, validation_data=val_dataset,epochs=100, callbacks = [early_stopping])\n",
    "\n",
    "# test_dataset = input_fn(test_feature_dict, epochs=1, shuffle=False, batch_size=1024, fit_key='predict')\n",
    "# age_prob = age_model.predict(test_dataset)\n",
    "# age_val_prob = age_model.predict(val_dataset)\n",
    "# print(age_prob.shape,age_val_prob.shape)\n",
    "# age_tune_weight = search_weight(age_val_label-1, age_val_prob, init_weight=[1.0]*10,class_num=10, step=0.001)\n",
    "# print(age_tune_weight)\n",
    "\n",
    "# age_prob_tune = np.array(age_tune_weight)*age_prob\n",
    "# age_pre = np.argmax(age_prob_tune,axis=1) + 1\n",
    "\n",
    "\n",
    "# np.save(f\"{path_sub}/val_age_prob.npy\", age_val_prob)\n",
    "# np.save(f\"{path_sub}/age_prob.npy\", age_prob)\n",
    "age_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gender_pre)\n",
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = range(3000001,4000001)\n",
    "sub['predicted_age'] = age_pre\n",
    "sub['predicted_gender'] = gender_pre\n",
    "print('ok！')\n",
    "sub.to_csv(f\"{sub_path}/submission.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fea in w2v_features:\n",
    "#     name = fea['name']\n",
    "#     max_length = fea['max_len']\n",
    "#     print(name)\n",
    "#     user_ids = np.load(f\"{path_list}{name}_list.npy\", allow_pickle=True)\n",
    "#     res = pd.Series(user_ids).map(lambda x:[int(i) for i in x])\n",
    "#     np.save(f\"{path_list}{name}_list_int.npy\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "units = 128\n",
    "age_model = lstm_model(units, num_classes, w2v_features, dense_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "creative_id (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "advertiser_id (InputLayer)      [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_id (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "industry (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_category (InputLayer)   [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 128, 256)     1138104832  creative_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 64)      4030080     advertiser_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 128, 64)      2836352     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 128, 32)      10880       industry[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 128, 16)      368         product_category[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 432)     0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 128, 256)     574464      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 256)     394240      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 256)     394240      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 256)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 256)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          65664       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1290        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,146,412,410\n",
      "Trainable params: 1,429,898\n",
      "Non-trainable params: 1,144,982,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "age_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_py3",
   "language": "python",
   "name": "conda_tensorflow2_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
