{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting tensorflow==2.1.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 421.8 MB 760 kB/s eta 0:00:011    |████▎                           | 56.2 MB 8.6 MB/s eta 0:00:43     |████▌                           | 59.1 MB 8.6 MB/s eta 0:00:43     |███████▎                        | 95.9 MB 3.0 MB/s eta 0:01:49     |██████████████                  | 183.8 MB 73.1 MB/s eta 0:00:04/s eta 0:00:04/s eta 0:00:04MB/s eta 0:00:03MB/s eta 0:00:03     |███████████████████▋            | 258.4 MB 72.0 MB/s eta 0:00:03��██            | 263.5 MB 72.0 MB/s eta 0:00:03██████████████▎           | 268.0 MB 72.0 MB/s eta 0:00:03     |█████████████████████           | 276.1 MB 69.7 MB/s eta 0:00:034 MB 760 kB/s eta 0:00:22\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.15.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/63/a5/e6c07b08b934831ccb8c98ee335e66b7761c5754ee3cabfe4c11d0b1af28/opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 17.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.12.3)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.18.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.7.1)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 55.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 74.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (47.3.1.post20200616)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/21/57/d706964a7e4056f3f2244e16705388c11631fbb53d3e2d2a2d0fbc24d470/google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 69.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.16.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/b3/59/524ffb454d05001e2be74c14745b485681c6ed5f2e625f71d135704c0909/cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 73.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 56.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 74.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 75.1 MB/s eta 0:00:01\n",
      "\u001b[31mERROR: tensorboard 2.1.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opt-einsum, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-estimator, scipy, tensorflow\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 0+untagged.56.g2664021.dirty\n",
      "    Uninstalling opt-einsum-0+untagged.56.g2664021.dirty:\n",
      "      Successfully uninstalled opt-einsum-0+untagged.56.g2664021.dirty\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.0\n",
      "    Uninstalling scipy-1.5.0:\n",
      "      Successfully uninstalled scipy-1.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed cachetools-4.1.0 google-auth-1.18.0 google-auth-oauthlib-0.4.1 oauthlib-3.1.0 opt-einsum-3.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.1.0\n",
    "# #!conda install -c anaconda cudatoolkit==10.1\n",
    "# #!conda config --set channel_priority flexible\n",
    "# #!conda install cudatoolkit=10.1.243 -c https://conda.anaconda.org/anaconda/linux-64\n",
    "\n",
    "# #!anaconda search -t conda cudatoolkit\n",
    "# !conda config --set show_channel_urls yes\n",
    "# # !conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "# # !conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge \n",
    "# # !conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/\n",
    "\n",
    "# !conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/\n",
    "# !conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/\n",
    "# !conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/\n",
    "# !conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/\n",
    "# !conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/\n",
    "# !conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/\n",
    "\n",
    "# !conda config --set show_channel_urls yes\n",
    "# !conda install cudatoolkit==10.1.243 -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /opt/conda/envs/tensorflow_py3/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/linux-64::tensorflow-gpu==1.15.0=h0d30ee6_0\n",
      "  - defaults/linux-64::tensorflow==1.15.0=gpu_py36h5a509aa_0\n",
      "  - defaults/linux-64::cupti==10.0.130=0\n",
      "  - defaults/noarch::tensorflow-estimator==1.15.1=pyh2649769_0\n",
      "  - defaults/linux-64::cudnn==7.6.5=cuda10.0_0\n",
      "  - defaults/linux-64::tensorflow-base==1.15.0=gpu_py36h9dcbed7_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/tensorflow_py3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudnn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cudatoolkit-10.0.130       |                0       261.2 MB  defaults\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       261.2 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  cudatoolkit             <unknown>::cudatoolkit-10.1.168-0 --> pkgs/main::cudatoolkit-10.0.130-0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!conda install --use-local cudatoolkit-10.1.168-0.tar.bz2\n",
    "#from utils import *\n",
    "#cos_to_local(\"/home/tione/notebook/cudatoolkit-10.1.168-0.tar.bz2\", \"lht-build-1259079995\", \"cudatoolkit-10.1.168-0.tar.bz2\")\n",
    "#!conda install cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n",
      "**************\n",
      "name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5473165850058337106\n",
      "\n",
      "-----------------------\n",
      "name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7612100997039069096\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "\n",
      "-----------------------\n",
      "name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2671863229401732333\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "\n",
      "-----------------------\n",
      "name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 24203077223\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17485924445581548977\n",
      "physical_device_desc: \"device: 0, name: Tesla P40, pci bus id: 0000:00:08.0, compute capability: 6.1\"\n",
      "\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#print(tf.__version__)\n",
    "#print(tf.test.is_built_with_gpu_support)\n",
    "#print(tf.test.is_gpu_available())\n",
    "#print(device_lib.list_local_devices())\n",
    "gpus = tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "print(gpus)\n",
    "print('**************')\n",
    "for i in device_lib.list_local_devices():\n",
    "    print(i)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "<function is_built_with_gpu_support at 0x7f66ceb9a6a8>\n",
      "True\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14254243081172726932\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8194619712690405421\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14596879618071472831\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 24203077223\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17910907575537035659\n",
      "physical_device_desc: \"device: 0, name: Tesla P40, pci bus id: 0000:00:08.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-034df82eebc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m#对需要进行限制的GPU进行设置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# tf.config.experimental.set_virtual_device_configuration(gpus[0],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "sys.path.append(r\"..\")\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,f1_score,recall_score\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "path_build = \"./build/\"\n",
    "path_embed = \"./embed/\"\n",
    "path_list = \"./feature_series/\"\n",
    "path_sub = \"./sub/\"\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_gpu_support)\n",
    "print(tf.test.is_gpu_available())\n",
    "print(device_lib.list_local_devices())\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#对需要进行限制的GPU进行设置\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#                                                       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "# gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load successfully!!!\n"
     ]
    }
   ],
   "source": [
    "creative_id_em = load_pickle(f\"{path_embed}/creative_id_w2v_matrix.pkl\")\n",
    "print('load successfully!!!')\n",
    "#ad_id_em = load_pickle(f\"{path_embed}/ad_id_w2v_matrix.pkl\")\n",
    "advertiser_id_em = load_pickle(f\"{path_embed}/advertiser_id_w2v_matrix.pkl\")\n",
    "product_id_em = load_pickle(f\"{path_embed}/product_id_w2v_matrix.pkl\")\n",
    "industry_em = load_pickle(f\"{path_embed}/industry_w2v_matrix.pkl\")\n",
    "product_category_em = load_pickle(f\"{path_embed}/product_category_w2v_matrix.pkl\")\n",
    "# click_times_em = load_pickle(f\"{path_embed}/click_times_w2v_matrix2.pkl\")\n",
    "# time_em = load_pickle(f\"{path_embed}/time_w2v_matrix2.pkl\")\n",
    "\n",
    "#creative_id_times_em = load_pickle(f\"{path_embed}/creative_id_times_w2v_matrix.pkl\")\n",
    "#ad_id_times_em = load_pickle(f\"{path_embed}/ad_id_times_w2v_matrix3.pkl\")\n",
    "#product_id_times_em = load_pickle(f\"{path_embed}/product_id_times_w2v_matrix.pkl\")\n",
    "#advertiser_id_times_em = load_pickle(f\"{path_embed}/advertiser_id_times_w2v_matrix.pkl\")\n",
    "#product_category_times_em = load_pickle(f\"{path_embed}/product_category_times_w2v_matrix.pkl\")\n",
    "#industry_times_em = load_pickle(f\"{path_embed}/industry_times_w2v_matrix.pkl\")\n",
    "\n",
    "# creative_id_t_em = load_pickle(f\"{path_save}/creative_id_t_w2v_matrix2.pkl\")\n",
    "# # ad_id_t_em = load_pickle(f\"{path_save}/ad_id_t_w2v_matrix2.pkl\")\n",
    "# product_id_t_em = load_pickle(f\"{path_save}/product_id_t_w2v_matrix2.pkl\")\n",
    "# advertiser_id_t_em = load_pickle(f\"{path_save}/advertiser_id_t_w2v_matrix2.pkl\")\n",
    "# product_category_t_em = load_pickle(f\"{path_save}/product_category_t_w2v_matrix2.pkl\")\n",
    "# industry_t_em = load_pickle(f\"{path_save}/industry_t_w2v_matrix2.pkl\")\n",
    "\n",
    "#time_clicktimes_em = load_pickle(f\"{path_embed}/time_clicktimes_w2v_matrix2.pkl\")\n",
    "#time_creativeids_em = load_pickle(f\"{path_embed}/time_creativeids_w2v_matrix2.pkl\")\n",
    "\n",
    "w2v_features = [\n",
    "    {'name':'creative_id', 'size':256, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':creative_id_em},\n",
    "    #{'name':'ad_id', 'size':128, 'windows':5, 'min_count':1, 'version':1, 'max_len':128, 'em':ad_id_em},\n",
    "    {'name':'advertiser_id', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':advertiser_id_em},\n",
    "    {'name':'product_id', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':product_id_em},\n",
    "    {'name':'industry', 'size':32, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':industry_em},\n",
    "    {'name':'product_category', 'size':16, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':product_category_em},\n",
    "#     {'name':'time', 'size':16, 'windows':5, 'min_count':1, 'version':1, 'max_len':128, 'em':time_em},\n",
    "#     {'name':'click_times', 'size':8, 'windows':5, 'min_count':1, 'version':1, 'max_len':128, 'em':click_times_em},\n",
    "    \n",
    "    #{'name':'creative_id_times', 'size':256, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':creative_id_times_em},\n",
    "    #{'name': 'ad_id_times', 'size': 128, 'windows': 10, 'min_count': 1, 'version': 1, 'max_len':128, 'em':ad_id_times_em},\n",
    "    #{'name':'product_id_times', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':product_id_times_em},\n",
    "    #{'name':'advertiser_id_times', 'size':64, 'windows':10, 'min_count':1, 'version':1, 'max_len':128, 'em':advertiser_id_times_em},\n",
    "    #{'name':'product_category_times', 'size':32, 'windows':10, 'min_count':1, 'version':1,'max_len':128, 'em':product_category_times_em},\n",
    "    #{'name':'industry_times', 'size':32, 'windows':10, 'min_count':1, 'version':1,'max_len':128, 'em':industry_times_em},   \n",
    "    \n",
    "    \n",
    "    #{'name':'time_clicktimes', 'size':91, 'windows':10, 'min_count':1, 'version':2, 'vocab_size':5000,'max_len':128,'em':time_clicktimes_em},\n",
    "    #{'name':'time_creativeids', 'size':91, 'windows':10, 'min_count':1, 'version':2,'vocab_size':5000, 'max_len':128,'em': time_creativeids_em},\n",
    "    \n",
    "]\n",
    "dense_features = ['creative_id_len', 'ad_id_len', 'product_id_len', 'product_category_len', 'advertiser_len', 'industry_len','time_len',\n",
    "                  'mean_clicktimes', 'max_clicktimes', 'min_clicktimes', 'mean_time', 'max_time', 'min_time']  \n",
    "dense_features = []\n",
    "base_features = ['creative_id', 'ad_id', 'product_id', 'product_category', 'advertiser_id', 'industry']\n",
    "base_features = ['creative_id', 'ad_id']\n",
    "\n",
    "for fea in base_features:\n",
    "    for g in [1]:\n",
    "        dense_features.append(f'mean_{fea}_gender_{g}')\n",
    "        #dense_features.append(f'sum_{fea}_gender_{g}')\n",
    "    for a in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        dense_features.append(f'mean_{fea}_age_{a}')\n",
    "        #dense_features.append(f'sum_{fea}_age_{a}')\n",
    "dense_features = []\n",
    "\n",
    "#print(creative_id_em.shape,ad_id_em.shape,advertiser_id_em.shape,product_id_em.shape)\n",
    "#print(industry_em.shape,product_category_em.shape,click_times_em.shape,time_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(units,num_classes,w2v_features, dense_features):\n",
    "    inputs_dict = dict()\n",
    "    embed_layer_list = []\n",
    "    for w2v_f in w2v_features:\n",
    "        em_name = w2v_f['name']\n",
    "        em_size = w2v_f['em'].shape[0]\n",
    "        em_dim = w2v_f['size']\n",
    "        em_m = w2v_f['em']\n",
    "        max_len = w2v_f['max_len']\n",
    "\n",
    "        inputs = keras.Input(shape=(max_len,), name=em_name)\n",
    "        inputs_dict[em_name] = inputs\n",
    "\n",
    "        embed_layer_list.append( keras.layers.Embedding(\n",
    "                em_size, em_dim, input_length=max_len, trainable=False, weights=[em_m],mask_zero=False)(inputs))\n",
    "    embed_output = keras.layers.concatenate(embed_layer_list, axis=-1)\n",
    "    #embed_output = keras.layers.Conv1D(512, 5, padding='same', kernel_initializer='normal', activation='relu')(embed_output)\n",
    "    \n",
    "    lstm_output = keras.layers.Bidirectional(keras.layers.LSTM(units,return_sequences=True))(embed_output)\n",
    "    \n",
    "    #lstm_output = layers.GlobalMaxPooling1D()(lstm_output)\n",
    "                                     \n",
    "    \n",
    "    lstm_output = layers.concatenate([layers.GlobalAveragePooling1D()(lstm_output),\n",
    "                                      layers.GlobalMaxPooling1D()(lstm_output),\n",
    "                                     ], axis=-1)\n",
    "    #lstm_output = layers.BatchNormalization()(lstm_output)\n",
    "    lstm_output = layers.Dropout(0.3)(lstm_output)\n",
    "    \n",
    "    fc = keras.layers.Dense(units, activation='relu')(lstm_output)\n",
    "    #数值型特征\n",
    "    numeric_list = []\n",
    "    for den_f in dense_features:\n",
    "        inputs = keras.Input(shape=(1,), name=den_f)\n",
    "        inputs_dict[den_f] = inputs\n",
    "        numeric_list.append(inputs)   \n",
    "    if dense_features != []:\n",
    "        numeric_output = keras.layers.concatenate(numeric_list, axis=-1)\n",
    "        lstm_numeric_output = keras.layers.concatenate([fc,numeric_output], axis=-1)\n",
    "    else:\n",
    "        lstm_numeric_output = fc\n",
    "    \n",
    "    \n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(lstm_numeric_output)\n",
    "    \n",
    "    \n",
    "    model = keras.Model(inputs=inputs_dict, outputs=outputs)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.001),\n",
    "              loss = keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(feature_dict, label=None, epochs=5, shuffle=True, batch_size=64, fit_key='train'):\n",
    "    if fit_key == 'train':\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((feature_dict, label))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((feature_dict))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(100*batch_size)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creative_id\n",
      "advertiser_id\n",
      "product_id\n",
      "industry\n",
      "product_category\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_pickle(f\"{path_build}/train_user.pkl\")\n",
    "users.sort_values(by=['user_id'], ascending=[True], inplace=True)\n",
    "users = users.reset_index(drop=True)\n",
    "\n",
    "fold_train = False\n",
    "train_split = [0,2700000]\n",
    "val_split = [2700000, 3000000]\n",
    "test_split = [3000000]\n",
    "train_feature_dict = dict()\n",
    "val_feature_dict = dict()\n",
    "test_feature_dict = dict()\n",
    "if fold_train:\n",
    "    kfolder = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "    kfold = kfolder.split(user_ids[0:900000])\n",
    "    fold_index = 2\n",
    "    FLOD_TRAIN = 1\n",
    "    for train_index, vali_index in kfold:\n",
    "        if fold_index <= FLOD_TRAIN:\n",
    "            fold_index += 1\n",
    "            continue\n",
    "        print(train_index, vali_index)\n",
    "        break\n",
    "    \n",
    "    for fea in w2v_features:\n",
    "        name = fea['name']\n",
    "        max_length = fea['max_len']\n",
    "        print(name)\n",
    "        user_ids = np.load(f\"{path_list}{name}_list_int.npy\", allow_pickle=True)        \n",
    "        train_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[train_index],value = 0,padding = 'post',maxlen = max_length )\n",
    "        val_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[vali_index],value = 0,padding = 'post',maxlen = max_length )\n",
    "        test_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[test_split[0]:],value = 0,padding = 'post',maxlen = max_length )\n",
    "#     for fea in dense_features:\n",
    "#         print(fea)\n",
    "#         train_feature_dict[fea] = user_ids[fea][train_index]\n",
    "#         val_feature_dict[fea] = user_ids[fea][vali_index]\n",
    "#         test_feature_dict[fea] = user_ids[fea][test_split[0]:]\n",
    "\n",
    "    gender_train_label = np.array(users['gender'][train_index])\n",
    "    gender_val_label = np.array(users['gender'][vali_index])\n",
    "\n",
    "    age_train_label = np.array(users['age'][train_index])\n",
    "    age_val_label = np.array(users['age'][vali_index])\n",
    "    \n",
    "else:\n",
    "    for fea in w2v_features:\n",
    "        name = fea['name']\n",
    "        max_length = fea['max_len']\n",
    "        print(name)\n",
    "        user_ids = np.load(f\"{path_list}{name}_list_int.npy\", allow_pickle=True)\n",
    "        train_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[train_split[0]:train_split[1]],value = 0,padding = 'post',maxlen = max_length )\n",
    "        val_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[val_split[0]:val_split[1]],value = 0,padding = 'post',maxlen = max_length )\n",
    "        test_feature_dict[name] = keras.preprocessing.sequence.pad_sequences(\n",
    "            user_ids[test_split[0]:],value = 0,padding = 'post',maxlen = max_length )\n",
    "        \n",
    "#     for fea in dense_features:\n",
    "#         print(fea)\n",
    "#         train_feature_dict[fea] = user_ids[fea][train_split[0]:train_split[1]]\n",
    "#         val_feature_dict[fea] = user_ids[fea][val_split[0]:val_split[1]]\n",
    "#         test_feature_dict[fea] = user_ids[fea][test_split[0]:]\n",
    "\n",
    "    gender_train_label = np.array(users['gender'][train_split[0]:train_split[1]])\n",
    "    gender_val_label = np.array(users['gender'][val_split[0]:val_split[1]])\n",
    "\n",
    "    age_train_label = np.array(users['age'][train_split[0]:train_split[1]])\n",
    "    age_val_label = np.array(users['age'][val_split[0]:val_split[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 105469 steps, validate for 293 steps\n",
      "   113/105469 [..............................] - ETA: 13:50:26 - loss: 0.2658 - accuracy: 0.8952"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ade6bf95735c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feature_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_train_label\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_feature_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_val_label\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgender_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "units = 128\n",
    "gender_model = lstm_model(units, num_classes, w2v_features, dense_features)\n",
    "#gender_model.summary()\n",
    "train_dataset = input_fn(train_feature_dict, gender_train_label-1, epochs=5, shuffle=True, batch_size=128)\n",
    "val_dataset = input_fn(val_feature_dict, gender_val_label-1, epochs=1, shuffle=False, batch_size=1024)\n",
    "gender_model.fit(train_dataset, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = input_fn(test_feature_dict, epochs=1, shuffle=False, batch_size=1024, fit_key='predict')\n",
    "gender_prob = gender_model.predict(test_dataset)\n",
    "gender_val_prob = gender_model.predict(val_dataset)\n",
    "print(gender_prob.shape,gender_val_prob.shape)\n",
    "tune_weight = search_weight(gender_val_label-1, gender_val_prob, init_weight=[1.0]*2,class_num=2, step=0.001)\n",
    "\n",
    "gender_prob_tune = np.array(tune_weight)*gender_prob\n",
    "gender_pre = np.argmax(gender_prob_tune,axis=1) + 1\n",
    "np.save(f\"{sub_path}/val_gender_prob.npy\", gender_val_prob)\n",
    "np.save(f\"{sub_path}/gender_prob.npy\", gender_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def age_model_run():\n",
    "num_classes = 10\n",
    "units = 128\n",
    "age_model = lstm_model(units, num_classes, w2v_features, dense_features)\n",
    "#age_model.summary()\n",
    "train_dataset = input_fn(train_feature_dict, age_train_label-1, epochs=6, shuffle=True, batch_size=128)\n",
    "val_dataset = input_fn(val_feature_dict, age_val_label-1, epochs=1, shuffle=False, batch_size=1024)\n",
    "age_model.fit(train_dataset, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = input_fn(test_feature_dict, epochs=1, shuffle=False, batch_size=1024, fit_key='predict')\n",
    "age_prob = age_model.predict(test_dataset)\n",
    "age_val_prob = age_model.predict(val_dataset)\n",
    "print(age_prob.shape,age_val_prob.shape)\n",
    "age_tune_weight = search_weight(age_val_label-1, age_val_prob, init_weight=[1.0]*10,class_num=10, step=0.001)\n",
    "print(age_tune_weight)\n",
    "\n",
    "age_prob_tune = np.array(age_tune_weight)*age_prob\n",
    "age_pre = np.argmax(age_prob_tune,axis=1) + 1\n",
    "\n",
    "\n",
    "np.save(f\"{sub_path}/val_age_prob.npy\", age_val_prob)\n",
    "np.save(f\"{sub_path}/age_prob.npy\", age_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['user_id'] = range(3000001,4000001)\n",
    "sub['predicted_age'] = age_pre\n",
    "sub['predicted_gender'] = gender_pre\n",
    "print('ok！')\n",
    "sub.to_csv(f\"{sub_path}/submission.csv\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
